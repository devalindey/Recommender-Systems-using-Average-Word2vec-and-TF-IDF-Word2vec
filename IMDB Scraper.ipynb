{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Scaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the IMDB webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Accept-Language\": \"en-US,en;q=0.5\"} # This bring us English-translated content from the URLs we’re requesting\n",
    "\n",
    "pages = np.arange(1, 10001, 250) # Using this to stores each of our new URLs\n",
    "\n",
    "movie_divs = [] # Emply list that will append the movie_div data\n",
    "\n",
    "for page in pages: \n",
    "  \n",
    "    # We use this to grab the contents of each URL\n",
    "    page = requests.get('https://www.imdb.com/search/title/?title_type=feature,tv_series&count=250&start=' + str(page) + '&ref_=adv_nxt', headers = headers)\n",
    "    \n",
    "    # This will grab the text contents of page and use the HTML parser\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # We use this to store all of the div containers with a class of lister-item mode-advanced\n",
    "    movie_div = soup.find_all('div', class_='lister-item mode-advanced')\n",
    "    \n",
    "    # Storing the the above data for various pages\n",
    "    movie_divs.append(movie_div)\n",
    "    \n",
    "    # The sleep() function will control the loop’s rate by pausing the execution of the loop for a specified amount of time\n",
    "    ## The randint(2,10) function will vary the amount of waiting time between requests for a number between 2-10 seconds\n",
    "    sleep(randint(2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the required information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "img_links =[]\n",
    "\n",
    "for movie_div in movie_divs:\n",
    "    \n",
    "    for container in movie_div:\n",
    "        \n",
    "        # Scraping the movie names\n",
    "        name = container.h3.a.text \n",
    "        titles.append(name)\n",
    "        \n",
    "        # Scraping the movie image links\n",
    "        img_link = container.find('img', class_=\"loadlate\").attrs['loadlate']\n",
    "        img_links.append(img_link)\n",
    "        \n",
    "        # Scraping the descriptions\n",
    "        description = container.find_all(\"p\", class_=\"text-muted\")[-1].text.lstrip()\n",
    "        descriptions.append(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring & exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data in a dataframe\n",
    "movies = pd.DataFrame({\n",
    "    'Movie': titles,\n",
    "    'ImgLink': img_links,\n",
    "    'Description': descriptions})\n",
    "\n",
    "# Exporting the data to an Excel file\n",
    "movies.to_excel('IMDB_Dataset.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
